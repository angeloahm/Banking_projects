{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages:\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter, MultipleLocator\n",
    "from pandas_datareader import data as pdr     \n",
    "from pathlib import Path\n",
    "\n",
    "# Import custom functions:\n",
    "from aux_functions import *\n",
    "from CallReports import CallReports\n",
    "from mappings import mappings\n",
    "\n",
    "# ignore warnings:\n",
    "warnings.simplefilter(\"ignore\", pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "# Set the theme for plots\n",
    "sns.set_theme(\n",
    "    style   =\"whitegrid\",   # light grid behind the bars\n",
    "    palette =\"pastel\"       # soft default colours\n",
    ")\n",
    "# Set serif font for plots\n",
    "plt.rcParams['font.family'] = 'serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/angel/Documents/Economics/Research/Banking Project/data/clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import information on IDRSSD and Charter Type:\n",
    "att_closed = pd.read_csv(f'{path}/CSV_ATTRIBUTES_CLOSED.csv', low_memory=False)\n",
    "att_active = pd.read_csv(f'{path}/CSV_ATTRIBUTES_ACTIVE.csv', low_memory=False)\n",
    "\n",
    "att_active.rename(columns={'#ID_RSSD': 'IDRSSD', \n",
    "                           'CHTR_TYPE_CD': 'Charter Type'}, inplace=True)\n",
    "att_active = att_active[['IDRSSD', 'Charter Type']]\n",
    "\n",
    "att_closed.rename(columns={'#ID_RSSD': 'IDRSSD', \n",
    "                           'CHTR_TYPE_CD': 'Charter Type'}, inplace=True)\n",
    "att_closed = att_closed[['IDRSSD', 'Charter Type']]\n",
    "\n",
    "# merge att_closed and att_active:\n",
    "att = pd.concat([att_closed, att_active])\n",
    "\n",
    "# Keep only commercial banks:\n",
    "commercial_banks = att[att['Charter Type']==200]['IDRSSD'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load DFF.csv file:\n",
    "rates = pd.read_csv(f'{path}/DFF.csv', low_memory=False)\n",
    "rates.rename(columns={'observation_date': 'Date', 'DFF':'FedFunds Rate'}, inplace=True)\n",
    "\n",
    "# Convert 'Date' to datetime format:\n",
    "rates['Date'] = pd.to_datetime(rates['Date'], format='%Y-%m-%d')\n",
    "rates['Date'] = rates['Date'] - pd.Timedelta(days=1)\n",
    "rates['FedFunds Rate'] = rates['FedFunds Rate']/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = CallReports(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define maturity variables:\n",
    "loans_mat_vars = [\n",
    "                'RCONA564', 'RCONA565', 'RCONA566', 'RCONA567', 'RCONA568', 'RCONA569',     # used\n",
    "                #'RCFDA564', 'RCFDA565', 'RCFDA566', 'RCFDA567', 'RCFDA568', 'RCFDA569',     # to be tested\n",
    "                # ------------------------------------------------------------------------------------------------\n",
    "                'RCFDA570', 'RCFDA571', 'RCFDA572', 'RCFDA573', 'RCFDA574', 'RCFDA575',     # used \n",
    "                #'RCONA570', 'RCONA571', 'RCONA572', 'RCONA573', 'RCONA574', 'RCONA575',     # to be tested  \n",
    "                ]\n",
    "\n",
    "securities_mat_vars = [\n",
    "             # --------------------------------------  Treasuries  --------------------------------------\n",
    "                'RCFDA549', 'RCFDA550', 'RCFDA551', 'RCFDA552', 'RCFDA553', 'RCFDA554',     # used\n",
    "                'RCONA549', 'RCONA550', 'RCONA551', 'RCONA552', 'RCONA553', 'RCONA554',     # to be tested\n",
    "            # --------------------------------------  MBS  --------------------------------------\n",
    "                'RCFDA555', 'RCFDA556', 'RCFDA557', 'RCFDA558', 'RCFDA559', 'RCFDA560',     # used\n",
    "                'RCONA555', 'RCONA556', 'RCONA557', 'RCONA558', 'RCONA559', 'RCONA560',     # to be tested\n",
    "                ]\n",
    "\n",
    "\n",
    "# define the list of variables that will be used\n",
    "vars = [\n",
    "             # ------------------------------------------------------------------------------------------------ \n",
    "             'Date', 'IDRSSD', 'Financial Institution Name',            # Identifier Variables\n",
    "             # ------------------------------------------------------------------------------------------------\n",
    "            'RCON2170', 'RCFD2170',                                     # Total Assets\n",
    "            'RCON3368', 'RCFD3368',                                     # QA Total Assets\n",
    "             # ------------------------------------------------------------------------------------------------\n",
    "             'RCON2122', 'RCFD2122',                                    # Total Loans\n",
    "             # ------------------------------------------------------------------------------------------------\n",
    "             'RCON2200',                                                # Total Deposits\n",
    "             'RCON2215',                                                # Transaction Deposits\n",
    "             'RCON6648',                                                # 0-100k\n",
    "             'RCON2604',                                                # 100k+ (old)\n",
    "             'RCONJ473',                                                # 100-250k (new)\n",
    "             'RCONJ474',                                                # 250k+ (new)\n",
    "             # ------------------------------------------------------------------------------------------------\n",
    "             'RCONF045', 'RCONF046', 'RCONF047', \n",
    "             'RCONF048', 'RCONF049', 'RCONF050',\n",
    "             'RCONF051', 'RCONF052', 'RCON3645',                        # Uninsured Deposits\n",
    "             # ------------------------------------------------------------------------------------------------\n",
    "             'RCON3210', 'RCFD3210',                                    # Total Equity Capital\n",
    "             'RCONB530', 'RCFDB530',                                    # AOCI\n",
    "             # ------------------------------------------------------------------------------------------------\n",
    "             'RCON1754', 'RCFD1754',                                    # HTM Securities Ammortized Cost\n",
    "             'RCFD1754_x', 'RCFD1754_y', 'RCON1754_x', 'RCON1754_y',\n",
    "             # ------------------------------------------------------------------------------------------------\n",
    "             'RCON1771', 'RCFD1771',                                    # HTM Securities Fair Value\n",
    "             # ------------------------------------------------------------------------------------------------\n",
    "             'RCON1772', 'RCFD1772',                                    # AFS Securities Ammortized Cost\n",
    "             # ------------------------------------------------------------------------------------------------\n",
    "             'RCONJJ34', 'RCFDJJ34',                                    \n",
    "             'RCONJA22', 'RCFDJA22',                                    # Booked Securities\n",
    "             # ------------------------------------------------------------------------------------------------\n",
    "             'RCFD1773_x', 'RCFD1773_y', 'RCON1773',                    # AFS Securities Fair Value\n",
    "             # ------------------------------------------------------------------------------------------------\n",
    "             'RCONB987',                                                # FF sold in domestic offices\n",
    "             'RCONB989', 'RCFDB989',                                    # Resell agreements                                  \n",
    "             # ------------------------------------------------------------------------------------------------\n",
    "             'RCFD0081', 'RCON0081',                                    # Cash and Balances Due from Depository Institutions\n",
    "             'RCFD0071', 'RCON0071',\n",
    "             'RCFD0010', 'RCON0010',\n",
    "             # ------------------------------------------------------------------------------------------------\n",
    "             'RIAD4508',                                                 # Interest Expenses\n",
    "             'RIADA517', 'RIADA518', 'RIAD0093',\n",
    "             'RIADHK03', 'RIADHK04', 'RIAD4073', 'RIAD4200', 'RIAD4185', \n",
    "             'RIAD4180', 'RIAD4172', 'RIAD4107', 'RIAD4340', 'RIAD4093'\n",
    "             ] \n",
    "\n",
    "# create a list putting together 'vars', 'loans_mat_vars', and 'securities_mat_vars':\n",
    "all_vars = vars + loans_mat_vars + securities_mat_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = cr.select_variables(all_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all the new variables:\n",
    "main = cr.construct_definitions(mappings=mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only commercial banks:\n",
    "main = main[main['IDRSSD'].isin(commercial_banks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge main with rates per 'Date':\n",
    "main = pd.merge(main, rates, on='Date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Insured Deposit Accounts' and 'Number of Insured Deposit Accounts'\n",
    "main['Insured Deposit Accounts'] = main['RCONF045'] + main['RCONF049']\n",
    "main['Number of Insured Deposit Accounts'] = main['RCONF046'] + main['RCONF050']\n",
    "\n",
    "# Create 'Uninsured Deposit Accounts' and 'Number of Uninsured Deposit Accounts'\n",
    "main['Uninsured Deposit Accounts'] = main['RCONF047'] + main['RCONF051']\n",
    "main['Number of Uninsured Deposit Accounts'] = main['RCONF048'] + main['RCONF052']\n",
    "\n",
    "# Correct insurance coverage threshold based on date:\n",
    "main['Insurance Coverage'] = np.where(main['Date'] < pd.Timestamp('2010-03-01'), 100, 250)\n",
    "\n",
    "# Recalculate 'Insured Deposits' considering coverage threshold:\n",
    "main['Insured Deposits'] = (\n",
    "    main['Insured Deposit Accounts'] +\n",
    "    main['Number of Uninsured Deposit Accounts'] * main['Insurance Coverage']\n",
    ")\n",
    "\n",
    "# Recalculate 'Uninsured Deposits':\n",
    "main['Uninsured Deposits'] = (\n",
    "    main['Uninsured Deposit Accounts'] -\n",
    "    main['Number of Uninsured Deposit Accounts'] * main['Insurance Coverage']\n",
    ")\n",
    "\n",
    "# Create 'Total Deposits 2':\n",
    "main['Total Deposits 2'] = main['Insured Deposits'] + main['Uninsured Deposits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute 'Time Deposits Expenses' considering that it accumulates over 'Year'. We want expenses per 'Date'. Use \n",
    "# groupby(['IDRSSD, 'Year']):\n",
    "main['Time Deposit Expenses'] = (\n",
    "    main.groupby(['IDRSSD', main['Date'].dt.year])['Time Deposit Expenses']\n",
    "    .diff().fillna(main['Time Deposit Expenses'])\n",
    ")\n",
    "\n",
    "# Do the same for 'Transaction Deposit Expenses':\n",
    "main['Transaction Deposit Expenses'] = (\n",
    "    main.groupby(['IDRSSD', main['Date'].dt.year])['Transaction Deposit Expenses']\n",
    "    .diff().fillna(main['Transaction Deposit Expenses'])\n",
    ")\n",
    "\n",
    "# Do the same for 'Savings Expenses':\n",
    "main['Savings Expenses'] = (\n",
    "    main.groupby(['IDRSSD', main['Date'].dt.year])['Savings Expenses']\n",
    "    .diff().fillna(main['Savings Expenses'])\n",
    ")\n",
    "\n",
    "# Do the same for 'Interest Expenses on Subordinated Debt':\n",
    "main['Interest Expenses on Subordinated Debt'] = (\n",
    "    main.groupby(['IDRSSD', main['Date'].dt.year])['Interest Expenses on Subordinated Debt']\n",
    "    .diff().fillna(main['Interest Expenses on Subordinated Debt'])\n",
    ")\n",
    "\n",
    "# Do the same for 'Interest Expenses on FFS':\n",
    "main['Interest Expenses on FFS'] = (\n",
    "    main.groupby(['IDRSSD', main['Date'].dt.year])['Interest Expenses on FFS']\n",
    "    .diff().fillna(main['Interest Expenses on FFS'])\n",
    ")\n",
    "\n",
    "# Do the same for 'Interest Expenses on Foreign Deposits':\n",
    "main['Interest Expenses on Foreign Deposits'] = (\n",
    "    main.groupby(['IDRSSD', main['Date'].dt.year])['Interest Expenses on Foreign Deposits']\n",
    "    .diff().fillna(main['Interest Expenses on Foreign Deposits'])\n",
    ")\n",
    "\n",
    "# Do the same for 'Total Interest Expenses':\n",
    "main['Total Interest Expenses'] = (\n",
    "    main.groupby(['IDRSSD', main['Date'].dt.year])['Total Interest Expenses']\n",
    "    .diff().fillna(main['Total Interest Expenses'])\n",
    ")\n",
    "\n",
    "# Do the same for 'Total Interest Income':\n",
    "main['Total Interest Income'] = (\n",
    "    main.groupby(['IDRSSD', main['Date'].dt.year])['Total Interest Income']\n",
    "    .diff().fillna(main['Total Interest Income'])\n",
    ")\n",
    "\n",
    "# Do the same for 'Net Interest Income':\n",
    "main['Net Interest Income'] = (\n",
    "    main.groupby(['IDRSSD', main['Date'].dt.year])['Net Interest Income']\n",
    "    .diff().fillna(main['Net Interest Income'])\n",
    ")\n",
    "\n",
    "# Create 'Deposit Expenses' as the sum of all deposit expenses:\n",
    "main['Deposit Expenses'] = (\n",
    "    main['Transaction Deposit Expenses'] +\n",
    "    main['Savings Expenses'] +\n",
    "    main['Time Deposit Expenses']\n",
    ")\n",
    "\n",
    "# Compute 'Deposit Expenses 2':\n",
    "main['Deposit Expenses 2'] = (\n",
    "    main['Total Interest Expenses'].fillna(0) - main['Interest Expenses on Subordinated Debt'].fillna(0) - \n",
    "    main['Other Interest Expenses'].fillna(0) - main['Interest Expenses on FFS'].fillna(0) - \n",
    "    main['Interest Expenses on Foreign Deposits'].fillna(0)\n",
    ")\n",
    "\n",
    "main['Non Interest Expenses'] = (\n",
    "    main.groupby(['IDRSSD', main['Date'].dt.year])['Non Interest Expenses']\n",
    "    .diff().fillna(main['Non Interest Expenses'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'Deposit Rates' as the ratio of 'Deposit Expenses' to 'Total Deposits':\n",
    "main['Deposit Rates'] = main['Deposit Expenses'] / main['Total Deposits']\n",
    "# mask negative values:\n",
    "main['Deposit Rates'] = main['Deposit Rates'].mask(main['Deposit Rates'] <= 0, np.nan)\n",
    "\n",
    "main['Deposit Rates 2'] = main['Deposit Expenses 2'] / main['Total Deposits']\n",
    "# mask negative values:\n",
    "main['Deposit Rates 2'] = main['Deposit Rates 2'].mask(main['Deposit Rates 2'] <= 0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new variables:\n",
    "main['Total Securities'] = main['HTM Securities'].fillna(0) + main['AFS Securities'].fillna(0) + main['Equity Securities'].fillna(0)\n",
    "main['Securities per Assets'] = main['Total Securities'] / main['Total Assets']\n",
    "main['Securities and FFS per Asset'] = (main['Total Securities'] + main['FFS']) / main['Total Assets']\n",
    "main['Cash, Securities and FFS per Asset'] = (main['Cash'] + main['Total Securities'] + main['FFS']) / main['Total Assets']\n",
    "main['Cash per Asset'] = main['Cash'] / main['Total Assets']\n",
    "# mask shares that are negative or greater than 1:\n",
    "main['Securities per Assets'] = main['Securities per Assets'].mask(main['Securities per Assets'] < 0, np.nan)\n",
    "main['Securities per Assets'] = main['Securities per Assets'].mask(main['Securities per Assets'] > 1, np.nan)\n",
    "main['Securities and FFS per Asset'] = main['Securities and FFS per Asset'].mask(main['Securities and FFS per Asset'] < 0, np.nan)\n",
    "main['Securities and FFS per Asset'] = main['Securities and FFS per Asset'].mask(main['Securities and FFS per Asset'] > 1, np.nan)\n",
    "main['Cash, Securities and FFS per Asset'] = main['Cash, Securities and FFS per Asset'].mask(main['Cash, Securities and FFS per Asset'] < 0, np.nan)\n",
    "main['Cash, Securities and FFS per Asset'] = main['Cash, Securities and FFS per Asset'].mask(main['Cash, Securities and FFS per Asset'] > 1, np.nan)\n",
    "main['Cash per Asset'] = main['Cash per Asset'].mask(main['Cash per Asset'] < 0, np.nan)\n",
    "main['Cash per Asset'] = main['Cash per Asset'].mask(main['Cash per Asset'] > 1, np.nan)\n",
    "\n",
    "# Create 'Interest Expenses Rate' as the ratio of expenses and Total Assets, mask the negatives\n",
    "main['Interest Expenses Rate'] = 4* main['Total Interest Expenses'] / main['QA Total Assets']\n",
    "main['Interest Expenses Rate'] = main['Interest Expenses Rate'].mask(main['Interest Expenses Rate'] < 0, np.nan)\n",
    "\n",
    "# Create 'Interest Income Rate' as the ratio of income and Total Assets, mask the negatives:\n",
    "main['Interest Income Rate'] = 4* main['Total Interest Income'] / main['QA Total Assets']\n",
    "main['Interest Income Rate'] = main['Interest Income Rate'].mask(main['Interest Income Rate'] < 0, np.nan)\n",
    "\n",
    "# Create 'Deposit Expenses Rate' as the ratio of expenses and Total Assets, mask the negatives:\n",
    "main['Deposit Expenses Rate'] = 4* main['Deposit Expenses 2'] / main['QA Total Assets']\n",
    "main['Deposit Expenses Rate'] = main['Deposit Expenses Rate'].mask(main['Deposit Expenses Rate'] < 0, np.nan)\n",
    "\n",
    "# Create 'Non Interest Expenses Rate' as the ratio of Non Interest Expenses and Total Assets, mask the negatives:\n",
    "main['Non Interest Expenses Rate'] = 4* main['Non Interest Expenses'] / main['QA Total Assets']\n",
    "main['Non Interest Expenses Rate'] = main['Non Interest Expenses Rate'].mask(main['Non Interest Expenses Rate'] < 0, np.nan)\n",
    "\n",
    "# Create 'Return on Assets' as the ratio of Net Interest Income and Total Assets, mask the negatives:\n",
    "main['Return on Assets'] = 4* main['Net Interest Income'] / main['QA Total Assets']\n",
    "main['Return on Assets'] = main['Return on Assets'].mask(main['Return on Assets'] < 0, np.nan)\n",
    "\n",
    "# Create 'NIM' as the difference between 'Interest Income Rate' and 'Interest Expenses Rate':\n",
    "main['NIM'] = main['Interest Income Rate'] - main['Interest Expenses Rate']\n",
    "\n",
    "# Create 'Leverage Ratio' as the ratio of Total Assets and Total Equity Capital:\n",
    "main['Leverage Ratio'] = main['Total Assets'] / main['Total Equity Capital']\n",
    "main['Leverage Ratio'] = main['Leverage Ratio'].mask(main['Leverage Ratio'] < 0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 'Treasury Maturity' variable:\n",
    "\n",
    "main['Treasury Maturity'] = ((\n",
    "                            main['Treasuries (3M-)']*1.5 + \n",
    "                            main['Treasuries (3M-1Y)']*7.5 + \n",
    "                            main['Treasuries (1Y-3Y)']*24 + \n",
    "                            main['Treasuries (3Y-5Y)']*48 + \n",
    "                            main['Treasuries (5Y-15Y)']*120 + \n",
    "                            main['Treasuries (15Y+)']*180\n",
    "                            ) / (\n",
    "                                main['Treasuries (3M-)']        + \n",
    "                                main['Treasuries (3M-1Y)']      + \n",
    "                                main['Treasuries (1Y-3Y)']      + \n",
    "                                main['Treasuries (3Y-5Y)']      + \n",
    "                                main['Treasuries (5Y-15Y)']     + \n",
    "                                main['Treasuries (15Y+)']\n",
    "                            )) / 12\n",
    "\n",
    "print(main['Treasury Maturity'].describe())\n",
    "\n",
    "# create the 'MBS Maturity' variable:\n",
    "main['MBS Maturity'] = ((\n",
    "                            main['MBS (3M-)']*1.5 + \n",
    "                            main['MBS (3M-1Y)']*7.5 + \n",
    "                            main['MBS (1Y-3Y)']*24 + \n",
    "                            main['MBS (3Y-5Y)']*48 + \n",
    "                            main['MBS (5Y-15Y)']*120 + \n",
    "                            main['MBS (15Y+)']*180\n",
    "                            ) / (\n",
    "                                main['MBS (3M-)']        + \n",
    "                                main['MBS (3M-1Y)']      + \n",
    "                                main['MBS (1Y-3Y)']      + \n",
    "                                main['MBS (3Y-5Y)']      + \n",
    "                                main['MBS (5Y-15Y)']     + \n",
    "                                main['MBS (15Y+)']\n",
    "                            )) / 12\n",
    "print(main['MBS Maturity'].describe())\n",
    "\n",
    "# create the 'Security Maturity' variable:\n",
    "main['Security Maturity'] = ((\n",
    "                            main['Securities (3M-)']*1.5 + \n",
    "                            main['Securities (3M-1Y)']*7.5 + \n",
    "                            main['Securities (1Y-3Y)']*24 + \n",
    "                            main['Securities (3Y-5Y)']*48 + \n",
    "                            main['Securities (5Y-15Y)']*120 + \n",
    "                            main['Securities (15Y+)']*180\n",
    "                            ) / (\n",
    "                                main['Securities (3M-)']        + \n",
    "                                main['Securities (3M-1Y)']      + \n",
    "                                main['Securities (1Y-3Y)']      + \n",
    "                                main['Securities (3Y-5Y)']      + \n",
    "                                main['Securities (5Y-15Y)']     + \n",
    "                                main['Securities (15Y+)']\n",
    "                            )) / 12\n",
    "\n",
    "print(main['Security Maturity'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main['High Rates'] = main['FedFunds Rate'] > main['FedFunds Rate'].quantile(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dreschler et. al (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = main[['Date', 'IDRSSD', 'Financial Institution Name', 'Total Interest Income', 'Total Interest Expenses',\n",
    "           'Interest Expenses on Subordinated Debt', 'Other Interest Expenses', 'Interest Expenses on FFS',\n",
    "           'Interest Expenses on Foreign Deposits', 'Deposit Expenses', 'Deposit Expenses 2', 'Deposit Rates',\n",
    "           'Deposit Rates 2', 'Total Deposits', 'Total Deposits 2', 'Insured Deposits', 'Uninsured Deposits',\n",
    "           'Total Assets', 'HTM Securities', 'AFS Securities', 'Equity Securities', 'FFS', 'Cash',\n",
    "           'Transaction Deposits', 'Time Deposits', 'Time Deposit Expenses', 'Transaction Deposit Expenses',\n",
    "           'Savings Expenses', 'Total Securities', 'Securities per Assets', 'Securities and FFS per Asset',\n",
    "           'Cash, Securities and FFS per Asset', 'Cash per Asset', 'Return on Assets', 'NIM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use seaborn to plot 'Total Interest Income' and 'Total Interest Expenses':\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(x='Date', y='Interest Income Rate', data=main, ax=ax, errorbar=None, ls='-.', lw=3)\n",
    "sns.lineplot(x='Date', y='Interest Expenses Rate', data=main, ax=ax, errorbar=None, color='firebrick', lw=3)\n",
    "sns.lineplot(x='Date', y='Deposit Expenses Rate', data=main, ax=ax, errorbar=None, lw=3, ls=':', color='black')\n",
    "plt.legend(labels=['Total Interest Income', 'Total Interest Expenses', 'Deposit Expenses'])\n",
    "plt.title('Total Interest Income and Total Interest Expenses')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel(None)\n",
    "# put y-ticks every 0.04 units:\n",
    "ax.yaxis.set_major_locator(MultipleLocator(0.04))\n",
    "plt.ylim(0, 0.16)\n",
    "plt.grid(True, linestyle=':', alpha=0.6, color='lightgray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put two plots side by side, the one above and NIM with ROA:\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sns.lineplot(x='Date', y='Return on Assets', data=main, ax=ax, errorbar=None, ls='-.', lw=3)\n",
    "sns.lineplot(x='Date', y='NIM', data=main, ax=ax, errorbar=None, ls='-', lw=3, color='firebrick')\n",
    "plt.legend(labels=['ROA', 'NIM'])\n",
    "plt.title('NIM and ROA')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel(None)\n",
    "plt.ylim(0, 0.16)\n",
    "plt.grid(True, linestyle=':', alpha=0.6, color='lightgray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begenau et. al (2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = main[['Date', 'IDRSSD', 'Financial Institution Name', \n",
    "           'Total Assets', 'HTM Securities', 'AFS Securities', 'FFS', 'Equity Securities', 'Cash',\n",
    "           'Transaction Deposits', 'Time Deposits', 'Time Deposit Expenses', 'Transaction Deposit Expenses', \n",
    "           'Cash, Securities and FFS per Asset', 'Cash per Asset', 'Securities per Assets', 'Securities and FFS per Asset',\n",
    "           'Total Securities']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep 2010Q1 to 2022Q4:\n",
    "df = df[(df['Date'] >= '2010-01-01') & (df['Date'] <= '2022-12-31')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Log Total Assets'] = np.log(df['Total Assets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a scatter plot of 'Cash, Securities and FFS per Asset' vs 'Total Assets' using seaborn:\n",
    "plt.figure(figsize=(8, 6))\n",
    "binned_scatter(df['Total Assets'], df['Cash, Securities and FFS per Asset'], q=100, marker='o', dispersion=False)\n",
    "plt.axvline(x=.995, color='lightcoral', linestyle='--', label='99th Percentile of Total Assets')\n",
    "plt.title('Cash, Securities and FFS per Asset vs Total Assets')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# make a scatter plot of 'Securities and FFS per Asset' vs 'Total Assets' using seaborn:\n",
    "plt.figure(figsize=(8, 6))\n",
    "binned_scatter(df['Total Assets'], df['Cash, Securities and FFS per Asset'], q=100, marker='o', dispersion=False, label='w/ cash')\n",
    "binned_scatter(df['Total Assets'], df['Securities and FFS per Asset'], q=100, marker='s', dispersion=False, label='w/o cash', color='firebrick')\n",
    "plt.legend()\n",
    "plt.axvline(x=.995, color='lightcoral', linestyle='--', label='99th Percentile of Total Assets')\n",
    "plt.title(\"Cross sectional distribution of banks' portfolios\")\n",
    "plt.legend(['w/ cash', 'w/o cash', '99th Percentile of Total Assets'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silicon Valley Event Study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import statsmodels.api as sm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/angel/Documents/Economics/Research/Banking Project/replications/svb_event_study'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\n",
    "    \"ALLY\",  # Ally Financial Inc.\n",
    "    \"AMTB\",  # Amerant Bancorp Inc.\n",
    "    \"ABCB\",  # Ameris Bancorp\n",
    "    \"ASB\",   # Associated Banc-Corp\n",
    "    \"AUB\",   # Atlantic Union Bankshares Corporation\n",
    "    \"AX\",    # Axos Financial Inc.\n",
    "    \"BANC\",  # Banc of California Inc.\n",
    "    \"BK\",    # Bank of New York Mellon Corporation\n",
    "    \"BAC\",   # Bank of America Corporation\n",
    "    \"BOH\",   # Bank of Hawaii Corporation\n",
    "    \"BKU\",   # BankUnited Inc.\n",
    "    \"BHB\",   # Bar Harbor Bankshares Inc.\n",
    "    \"BHLB\",  # Berkshire Hills Bancorp Inc.\n",
    "    \"BRBS\",  # Blue Ridge Bankshares Inc.\n",
    "    \"CADE\",  # Cadence Bank\n",
    "    \"COF\",   # Capital One Financial Corporation\n",
    "    \"C\",     # Citigroup Inc.\n",
    "    \"CFG\",   # Citizens Financial Group Inc.\n",
    "    \"CMA\",   # Comerica Incorporated\n",
    "    \"CFR\",   # Cullen/Frost Bankers Inc. \n",
    "    \"FNB\",   # F.N.B. Corporation\n",
    "    \"FBK\",   # FB Financial Corporation\n",
    "    \"FITB\",  # Fifth Third Bancorp\n",
    "    \"HBAN\",  # Huntington Bancshares Incorporated\n",
    "    \"KEY\",   # KeyCorp\n",
    "    \"MTB\",   # M&T Bank Corporation\n",
    "    \"PNC\",   # PNC Financial Services Group Inc.\n",
    "    \"RF\",    # Regions Financial Corporation\n",
    "    \"STT\",   # State Street Corporation\n",
    "    \"SYF\",   # Synchrony Financial\n",
    "    \"USB\",   # U.S. Bancorp\n",
    "    \"WFC\",   # Wells Fargo & Company\n",
    "    \"SCHW\",  # Charles Schwab Corporation\n",
    "    \"AXP\",   # American Express Company\n",
    "    \"DFS\",   # Discover Financial Services\n",
    "    \"NTB\",   # Bank of N.T. Butterfield & Son Limited\n",
    "    \"EWBC\",  # East West Bancorp Inc.\n",
    "    \"WAL\",   # Western Alliance Bancorporation\n",
    "    \"SSB\",   # SouthState Corporation\n",
    "    \"WBS\",   # Webster Financial Corporation\n",
    "    \"FHN\",   # First Horizon Corporation\n",
    "    \"PNFP\",  # Pinnacle Financial Partners Inc.\n",
    "    \"HOMB\",  # Home BancShares Inc.\n",
    "    \"HTH\",   # Hilltop Holdings Inc.\n",
    "    \"GBCI\",  # Glacier Bancorp Inc.\n",
    "    \"BOKF\",  # BOK Financial Corporation\n",
    "    \"ZION\",  # Zions Bancorporation\n",
    "    \"TCBI\",  # Texas Capital Bancshares Inc.\n",
    "    \"CIVB\",  # Civista Bancshares Inc.\n",
    "    \"CFFI\",  # C&F Financial Corporation\n",
    "    \"BANF\",  # BancFirst Corporation\n",
    "    \"FULT\",  # Fulton Financial Corporation\n",
    "    \"ONB\",   # Old National Bancorp\n",
    "    \"PB\",    # Prosperity Bancshares Inc.\n",
    "    \"UBSI\",  # United Bankshares Inc.\n",
    "    \"VLY\",   # Valley National Bancorp\n",
    "    \"TRMK\",  # Trustmark Corporation \n",
    "    \"CASH\",  # Meta Financial Group Inc.\n",
    "    \"CUBI\",  # Customers Bancorp Inc.\n",
    "    \"CFFN\",  # Capitol Federal Financial Inc.\n",
    "    \"FFIN\",  # First Financial Bankshares Inc. \n",
    "    \"SFBS\",  # ServisFirst Bancshares Inc.\n",
    "    \"TBBK\",  # The Bancorp Inc. \n",
    "    \"WSBC\",  # WesBanco Inc.\n",
    "    \"WTFC\",  # Wintrust Financial Corporation\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_date = pd.Timestamp(\"2023-03-08\")\n",
    "event_window = pd.Timedelta(days=10)  \n",
    "estimation_window = pd.Timedelta(days=500) \n",
    "start_date = event_date - event_window - estimation_window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tickers and rename columns to only retain ticker names\n",
    "stock_data = yf.download(\n",
    "    tickers, \n",
    "    start=start_date, \n",
    "    end=event_date + 4*event_window,\n",
    "    progress=False,\n",
    ")\n",
    "stock_data = stock_data.iloc[:, 0:len(tickers)]\n",
    "stock_data.columns = (\n",
    "    stock_data.columns.map(lambda col: re.sub(r\"[()\\s']\", \"\", col[1]))\n",
    ")\n",
    "\n",
    "# Calculate daily returns\n",
    "returns = stock_data.pct_change().dropna()*100\n",
    "\n",
    "# Read in FF 3 factor daily data\n",
    "ff_path = path + '/fama-french-3.csv'\n",
    "fama_french_data = pd.read_csv(ff_path).iloc[:, :-1]\n",
    "# Read the Date column as a date and set it as the index\n",
    "fama_french_data[\"Date\"] = pd.to_datetime(\n",
    "    fama_french_data[\"Date\"], \n",
    "    format=\"%Y-%m-%d\", \n",
    ")\n",
    "fama_french_data = fama_french_data.set_index(\"Date\")\n",
    "\n",
    "# Merge returns and FF data, drop unnecessary data\n",
    "merged_data = pd.concat(\n",
    "    [returns, fama_french_data], \n",
    "    axis=1\n",
    ").dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays to hold abnormal returns   \n",
    "abnormal_returns_factor = pd.DataFrame()\n",
    "abnormal_returns_mean = pd.DataFrame()    \n",
    "\n",
    "# Estimate abnormal returns for each \n",
    "for ticker in tickers:\n",
    "    # Estimate the 3-factor abnormal returns\n",
    "    X = merged_data.loc[\n",
    "        merged_data.index < event_date - event_window, \n",
    "        [\"Mkt-RF\", \"SMB\", \"HML\"]\n",
    "    ]   \n",
    "    y = merged_data.loc[\n",
    "        merged_data.index < event_date - event_window, \n",
    "        ticker\n",
    "    ] \n",
    "    X = sm.add_constant(X) \n",
    "    model = sm.OLS(y, X).fit()   \n",
    "\n",
    "    # Compute expected returns during the window\n",
    "    X_event = (\n",
    "        merged_data.loc[\n",
    "            ((merged_data.index >= event_date - event_window) & \n",
    "            (merged_data.index <= event_date + event_window)), \n",
    "            [\"Mkt-RF\", \"SMB\", \"HML\"]\n",
    "        ]\n",
    "    )\n",
    "    X_event = sm.add_constant(X_event)\n",
    "    expected_returns = model.predict(X_event)\n",
    "\n",
    "    # Extract realized returns during the event\n",
    "    event_data = (\n",
    "        merged_data.loc[\n",
    "            ((merged_data.index >= event_date - event_window) & \n",
    "            (merged_data.index <= event_date + event_window)), \n",
    "            ticker\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Abnormal returns are residuals\n",
    "    abnormal_returns_factor[ticker] = event_data - expected_returns\n",
    "    abnormal_returns_mean[ticker] = event_data - y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array for day dummies\n",
    "dummy_df = (\n",
    "    pd.get_dummies(\n",
    "        np.maximum(\n",
    "            (abnormal_returns_mean.index - pd.to_datetime(event_date)).days+1, \n",
    "            0,\n",
    "        ), \n",
    "        prefix='day',\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set the index again\n",
    "dummy_df.index = abnormal_returns_factor.index\n",
    "# Drop the column of days preceding the event\n",
    "dummy_df = dummy_df.drop(\"day_0\", axis=1)\n",
    "\n",
    "# Melt abnormal returns data\n",
    "ar_factor_long = abnormal_returns_factor.reset_index().melt(id_vars=\"Date\")\n",
    "ar_factor_long.head()\n",
    "\n",
    "# Merge data\n",
    "ar_factor_days = pd.merge(\n",
    "    ar_factor_long, \n",
    "    dummy_df, \n",
    "    how='left', \n",
    "    left_on='Date', \n",
    "    right_index=True,\n",
    ")\n",
    "# Split into two\n",
    "exog = ar_factor_days.filter(regex=\"day*\", axis=1).astype('float64')\n",
    "exog = sm.add_constant(exog)\n",
    "endog = ar_factor_days.loc[:, \"value\"]\n",
    "\n",
    "# View\n",
    "ar_factor_days.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_model = sm.OLS(endog, exog).fit(cov_type=\"HC0\")\n",
    "print(factor_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefs and SEs of daily indicators\n",
    "dummy_coefficients = factor_model.params.iloc[1:]  \n",
    "dummy_cov_matrix = factor_model.cov_params().iloc[1:, 1:]  \n",
    "\n",
    "# Compute the cumulative effects\n",
    "cumulative_effects = dummy_coefficients.cumsum()\n",
    "\n",
    "# Compute the standard errors of the cumulative effects\n",
    "cumulative_se = np.zeros(len(cumulative_effects))\n",
    "for t in range(1, len(cumulative_effects) + 1):\n",
    "    # Create a vector of ones up to day t and zeros afterwards\n",
    "    indicator_vector = np.concatenate(\n",
    "        [np.ones(t), np.zeros(len(cumulative_effects) - t)]\n",
    "    )\n",
    "\n",
    "    # Compute the SE of the cumulative effect\n",
    "    cumulative_se[t - 1] = np.sqrt(\n",
    "        indicator_vector @ dummy_cov_matrix @ indicator_vector\n",
    "    )\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results = pd.DataFrame(\n",
    "    {\n",
    "        \"Day\": factor_model.params.index[1:].map(lambda x: x[4:]),\n",
    "        \"Cumulative Effect\": cumulative_effects,\n",
    "        \"SE\": cumulative_se,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Prepend day 0 as a reference\n",
    "day0_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Day\": 0,\n",
    "        \"Cumulative Effect\": 0,\n",
    "        \"SE\": 0,\n",
    "    },\n",
    "    index = np.array(['day_0'])\n",
    ")\n",
    "results = pd.concat([day0_df, results])\n",
    "\n",
    "# Construct confidence intervals\n",
    "results[\"ci_upper\"] = results[\"Cumulative Effect\"] + 1.96 * results[\"SE\"]\n",
    "results[\"ci_lower\"] = results[\"Cumulative Effect\"] - 1.96 * results[\"SE\"]\n",
    "\n",
    "# Set index\n",
    "results.index = abnormal_returns_mean.index[6:]\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cumulative effects with confidence intervals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results.index, results[\"Cumulative Effect\"], marker='o', label='Cumulative Effect', lw=3, color='navy')\n",
    "plt.axhline(0, color='black', linestyle='--', lw=1, label='Zero Line')\n",
    "plt.title('Cumulative Abnormal Returns Around the Event Date')\n",
    "plt.xlabel('Days Relative to Event Date')\n",
    "plt.ylabel('Cumulative Abnormal Returns (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle=':', alpha=0.6, color='lightgray')\n",
    "plt.fill_between(\n",
    "    results.index, \n",
    "    results[\"ci_lower\"], \n",
    "    results[\"ci_upper\"], \n",
    "    color='lightgray', \n",
    "    alpha=0.5, \n",
    "    label='95% Confidence Interval'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paul GP, et. al (2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path          # nicer, OS-agnostic paths\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\n",
    "    r\"C:/Users/angel/Documents/Economics/Research/Banking Project/replications/bank_returns\"\n",
    ")\n",
    "os.chdir(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Analysis dates ------------------------------------------------------\n",
    "date_list   = [pd.Timestamp(\"2023-03-17\"),   # equivalent to R’s as.Date()\n",
    "               pd.Timestamp(\"2023-05-25\")]\n",
    "early_date, late_date = date_list            # unpack first / second element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Load data -----------------------------------------------------------\n",
    "# 3a. PERMCO ↔ RSSDID cross-walk\n",
    "permco_rssdid_xwalk = (\n",
    "    pd.read_csv(BASE_DIR / \"data\" / \"input\" / \"permco_rssdid_xwalk.csv\", encoding_errors='ignore')\n",
    "      .query(\"entity.notna()\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1426 PERMCO codes to C:\\Users\\angel\\Documents\\Economics\\Research\\Banking Project\\replications\\bank_returns\\data\\input\\permco_list.txt\n"
     ]
    }
   ],
   "source": [
    "# Write a .txt file with the unique PERMCO codes to be given to the WRDS. \n",
    "permcos = permco_rssdid_xwalk[\"permco\"].dropna().astype(int).unique()\n",
    "\n",
    "out_file = Path.cwd() / \"data\" / \"input\" / \"permco_list.txt\"\n",
    "with open(out_file, \"w\") as f:\n",
    "    for pc in permcos:\n",
    "        f.write(f\"{pc}\\n\")\n",
    "\n",
    "print(f\"Wrote {len(permcos)} PERMCO codes to {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b. WRDS – Compustat firm-level descriptors\n",
    "wrds_compustat_data = pd.read_csv(\n",
    "    BASE_DIR / \"data\" / \"input\" / \"wrds_data.csv\", encoding_errors='ignore'\n",
    ")\n",
    "\n",
    "wrds_compustat_data.rename(columns={'LPERMCO': 'permco'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Build the working sample ------------------------------------------\n",
    "#\n",
    "#  • inner-join the cross-walk (Permco ↔ RSSDID) with your freshly-downloaded\n",
    "#    WRDS securities file (Permco ↔ Ticker, etc.)\n",
    "#  • keep only records whose ‘dt_end’ ≥ 30-Sep-2021\n",
    "#  • drop the duplicate RSSDIDs called out in the authors’ note\n",
    "#\n",
    "\n",
    "# 1. Merge on PERMCO ---------------------------------\n",
    "bank_sample = (\n",
    "    permco_rssdid_xwalk                     # left table  (has ‘permco’, ‘entity’)\n",
    "      .merge(\n",
    "          wrds_compustat_data,              # right table (has ‘PERMCO’, ‘tic’, …)\n",
    "          how=\"inner\",\n",
    "          on=\"permco\",\n",
    "          )\n",
    ")\n",
    "\n",
    "# make 'entity' an integer:\n",
    "bank_sample['entity'] = bank_sample['entity'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Keep quarters from 2021-Q3 onward ---------------\n",
    "#    ‘dt_end’ arrives as an 8-digit int (20210930).  We can compare as int,\n",
    "#    or turn it into a timestamp if you prefer.\n",
    "bank_sample = bank_sample[bank_sample[\"dt_end\"] >= 20210930]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Drop the unwanted duplicates --------------------\n",
    "#    • entity 497570  = Luther Burbank *subsidiary* RSSDID\n",
    "#    • entity 3382332 = South Plains *stock plan* RSSDID\n",
    "bank_sample = bank_sample[~bank_sample[\"entity\"].isin([497570, 3382332])]\n",
    "\n",
    "# Optional: reset the index so it’s clean\n",
    "bank_sample = bank_sample.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Load the 2021-Q4 Y-9C filing (Schedule BHCF) -----------------------\n",
    "#\n",
    "# • File comes from the FR Y-9C bulk download.\n",
    "# • CARET (^) is the field delimiter used by the Fed’s flat files.\n",
    "# • We read it into a pandas DataFrame called y9c_data_2022q4\n",
    "\n",
    "y9c_data_2022q4 = pd.read_csv(\n",
    "    BASE_DIR / \"data\" / \"input\" / \"BHCF20221231.txt\",\n",
    "    sep=\"^\",               # same as data.table::fread(..., sep = \"^\")\n",
    "    engine=\"python\",       # python engine handles quirky delimiters better\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Enrich the 2021-Q4 Y-9C data and join to the bank sample ----------\n",
    "\n",
    "# 6-A.  Derive balance-sheet building blocks -------------------------------\n",
    "y9c_q4_aug = y9c_data_2022q4.copy()\n",
    "\n",
    "# helper: quick numeric coercion (Fed flat files arrive as object/str)\n",
    "to_num = lambda s: pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "# cash + securities buckets -------------------------------------------------\n",
    "y9c_q4_aug[\"cash_securities\"] = y9c_q4_aug[\n",
    "    [\"BHCK0081\", \"BHCK0395\", \"BHCK0397\", \"BHCKJJ34\", \"BHCK1773\", \"BHCKJA22\"]\n",
    "].apply(to_num).sum(axis=1, skipna=True)\n",
    "\n",
    "y9c_q4_aug[\"cash\"] = y9c_q4_aug[[\"BHCK0081\", \"BHCK0395\", \"BHCK0397\"]] \\\n",
    "    .apply(to_num).sum(axis=1, skipna=True)\n",
    "\n",
    "y9c_q4_aug[\"securities\"] = y9c_q4_aug[[\"BHCKJJ34\", \"BHCK1773\", \"BHCKJA22\"]] \\\n",
    "    .apply(to_num).sum(axis=1, skipna=True)\n",
    "\n",
    "# core line items -----------------------------------------------------------\n",
    "y9c_q4_aug[\"liabilties\"]            = to_num(y9c_q4_aug[\"BHCK2948\"])\n",
    "y9c_q4_aug[\"dep_y9c\"]               = y9c_q4_aug[[\"BHDM6631\", \"BHDM6636\"]] \\\n",
    "                                        .apply(to_num).sum(axis=1, skipna=True)\n",
    "y9c_q4_aug[\"nonperforming_loans\"]   = y9c_q4_aug[[\"BHCK1407\", \"BHCK1403\"]] \\\n",
    "                                        .apply(to_num).sum(axis=1, skipna=True)\n",
    "y9c_q4_aug[\"total_loans\"]           = y9c_q4_aug[[\"BHCK5369\", \"BHCKB528\"]] \\\n",
    "                                        .apply(to_num).sum(axis=1, skipna=True)\n",
    "\n",
    "# four-type deposit total (matches the authors’ as.numeric() + …) ----------\n",
    "y9c_q4_aug[\"deposits\"] = (\n",
    "      to_num(y9c_q4_aug[\"BHDM6631\"])\n",
    "    + to_num(y9c_q4_aug[\"BHDM6636\"])\n",
    "    + to_num(y9c_q4_aug[\"BHFN6631\"])\n",
    "    + to_num(y9c_q4_aug[\"BHFN6636\"])\n",
    ")\n",
    "\n",
    "# capital & ratios ----------------------------------------------------------\n",
    "y9c_q4_aug[\"tier1_capital\"] = to_num(y9c_q4_aug[\"BHCA8274\"])\n",
    "y9c_q4_aug[\"tier1capratio\"] = (to_num(y9c_q4_aug[\"BHCA7206\"])\n",
    "                               .fillna(to_num(y9c_q4_aug[\"BHCW7206\"])))\n",
    "\n",
    "# HTM / AFS positions -------------------------------------------------------\n",
    "y9c_q4_aug[\"htm\"]                     = to_num(y9c_q4_aug[\"BHCKJJ34\"])\n",
    "y9c_q4_aug[\"htm_book\"]                = to_num(y9c_q4_aug[\"BHCK1754\"])\n",
    "y9c_q4_aug[\"unrealized_htm_losses\"]   = (\n",
    "    to_num(y9c_q4_aug[\"BHCK1754\"]) - to_num(y9c_q4_aug[\"BHCK1771\"])\n",
    ")\n",
    "y9c_q4_aug[\"assets\"]                  = to_num(y9c_q4_aug[\"BHCK2170\"])\n",
    "y9c_q4_aug[\"afs\"]                     = to_num(y9c_q4_aug[\"BHCK1773\"])\n",
    "\n",
    "# keep only the columns the R script later references ----------------------\n",
    "keep_cols = [\n",
    "    \"deposits\", \"tier1_capital\", \"BHCA8274\", \"cash_securities\",\n",
    "    \"unrealized_htm_losses\", \"cash\", \"assets\", \"afs\",\n",
    "    \"htm\", \"htm_book\", \"securities\", \"liabilties\", \"dep_y9c\",\n",
    "    \"RSSD9001\", \"nonperforming_loans\", \"total_loans\", \"tier1capratio\"\n",
    "]\n",
    "y9c_q4_aug = y9c_q4_aug[keep_cols]\n",
    "\n",
    "# 6-B.  Merge with the earlier bank_sample ---------------------------------\n",
    "bank_q4 = (\n",
    "    bank_sample\n",
    "      .merge(y9c_q4_aug, left_on=\"entity\", right_on=\"RSSD9001\", how=\"inner\")\n",
    ")\n",
    "\n",
    "# 6-C.  Compute the ratios the paper tracks --------------------------------\n",
    "bank_q4 = bank_q4.assign(\n",
    "    htm_ratio                     = lambda df: df[\"htm\"] / df[\"assets\"],\n",
    "    htm_ratio2                    = lambda df: df[\"htm\"] / df[\"afs\"],\n",
    "    npl_ratio                     = lambda df: df[\"nonperforming_loans\"] / df[\"total_loans\"],\n",
    "    unrealized_htm_losses_rat_book   = lambda df: df[\"unrealized_htm_losses\"] / df[\"htm_book\"],\n",
    "    unrealized_htm_losses_rat_tier1  = lambda df: df[\"unrealized_htm_losses\"] / df[\"tier1_capital\"],\n",
    "    liquid_asset_ratio            = lambda df: df[\"cash_securities\"] / df[\"assets\"],\n",
    "    liquid_asset_ratio2           = lambda df: df[\"cash_securities\"] / df[\"tier1_capital\"],\n",
    "    cash_asset_ratio              = lambda df: df[\"cash\"] / df[\"assets\"],\n",
    "    securities_asset_ratio        = lambda df: df[\"securities\"] / df[\"assets\"],\n",
    "    loans_asset_ratio             = lambda df: df[\"total_loans\"] / df[\"assets\"],\n",
    "    deposits_liabilities_ratio    = lambda df: df[\"deposits\"] / df[\"liabilties\"],\n",
    ")\n",
    "\n",
    "# final clean-up: drop rows with missing total assets -----------------------\n",
    "bank_q4 = bank_q4[bank_q4[\"assets\"].notna()].reset_index(drop=True)\n",
    "\n",
    "#! This cell can be done nicely with our dictionary ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALL_DIR = (\n",
    "    BASE_DIR\n",
    "    / \"data\"\n",
    "    / \"input\"\n",
    "    / \"FFIEC CDR Call Bulk All Schedules 12312022\"\n",
    ")\n",
    "\n",
    "# 1. List all .txt files in the directory (same as dir(..., pattern = \".txt\"))\n",
    "files = sorted(CALL_DIR.glob(\"*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Read them and left-merge on IDRSSD, mimicking the R for-loop\n",
    "def read_schedule(fp: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read one FFIEC bulk Call-Report schedule.\n",
    "\n",
    "    * 2022-Q4 files are TAB-delimited (`\\t`) and Win-1252 encoded.\n",
    "    * Some lines contain stray tabs inside quotes → need a tolerant parser.\n",
    "    * If pyarrow is present, we use it (fast, streams well); otherwise the\n",
    "      pure-Python engine.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            fp,\n",
    "            sep='\\t',\n",
    "            encoding='cp1252',  # Windows-1252 encoding\n",
    "            low_memory=False,\n",
    "        ).drop(index=0).reset_index(drop=True)\n",
    "        df['IDRSSD'] = (\n",
    "                        pd.to_numeric(df['IDRSSD'], errors='coerce')   # turns bad strings into NaN\n",
    "                        .astype('Int64')                             # keeps missing as <NA>\n",
    "                        )\n",
    "        return df\n",
    "    except pd.errors.ParserError as err:\n",
    "        print(f'ParserError in {fp} -> {err}')\n",
    "        # In this case, use csv.QUOTE_NONE\n",
    "        df = pd.read_csv(\n",
    "            fp,\n",
    "            sep='\\t',\n",
    "            engine='python',  # use the Python engine for better error handling\n",
    "            encoding='cp1252',  # Windows-1252 encoding\n",
    "            quoting=csv.QUOTE_NONE,\n",
    "        ).drop(index=0).reset_index(drop=True)\n",
    "        # remove quotes from df:\n",
    "        df = df.replace({'\"': ''}, regex=True)\n",
    "        # remove quotes from column names:\n",
    "        df.columns = df.columns.str.replace('\"', '', regex=False)\n",
    "        df['IDRSSD'] = (\n",
    "                        pd.to_numeric(df['IDRSSD'], errors='coerce')   # turns bad strings into NaN\n",
    "                        .astype('Int64')                             # keeps missing as <NA>\n",
    "                        )\n",
    "\n",
    "\n",
    "\n",
    "# read the first file\n",
    "call_report_data_vars = read_schedule(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteratively left-join the remaining 47 (R used files[2:48])\n",
    "for fp in files[1:48]:\n",
    "    if \"NARR\" in fp.name:       # ⟵ skip the trouble-maker\n",
    "        continue\n",
    "    call_report_data_vars = call_report_data_vars.merge(\n",
    "        read_schedule(fp),\n",
    "        on=\"IDRSSD\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_y\"),\n",
    "        copy=False\n",
    "    )\n",
    "\n",
    "# ↪  call_report_data_vars now contains one wide row per RSSDID,\n",
    "#    with every column that appears in any of the 48 schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Build 2022-Q4 Call-Report subsample --------------------------------\n",
    "\n",
    "# helper so we type less\n",
    "num = lambda s: pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "# 8-A.  Derive the composite balance-sheet lines ---------------------------\n",
    "cr_aug = call_report_data_vars.copy()\n",
    "\n",
    "# cash ---------------------------------------------------------------------\n",
    "cash_fd  = num(cr_aug[\"RCFD0081\"]) + num(cr_aug[\"RCFD0071\"])\n",
    "cash_con = num(cr_aug[\"RCON0081\"]) + num(cr_aug[\"RCON0071\"])\n",
    "cr_aug[\"cash\"] = cash_fd.combine_first(cash_con)        # coalesce()\n",
    "\n",
    "# securities ---------------------------------------------------------------\n",
    "sec_fd  = num(cr_aug[\"RCFDJJ34\"]) + num(cr_aug[\"RCFD1773\"]) + num(cr_aug[\"RCFDJA22\"])\n",
    "sec_con = num(cr_aug[\"RCONJJ34\"]) + num(cr_aug[\"RCON1773\"]) + num(cr_aug[\"RCONJA22\"])\n",
    "cr_aug[\"securities\"] = sec_fd.combine_first(sec_con)\n",
    "cr_aug[\"cash_securities\"] = cr_aug[\"cash\"] + cr_aug[\"securities\"]\n",
    "\n",
    "# liabilities, NPLs, loans --------------------------------------------------\n",
    "cr_aug[\"liabilties\"] = num(cr_aug[\"RCFD2948\"]).combine_first(num(cr_aug[\"RCON2948\"]))\n",
    "\n",
    "npl_fd  = num(cr_aug[\"RCFD1407\"]) + num(cr_aug[\"RCFD1403\"])\n",
    "npl_con = num(cr_aug[\"RCON1407\"]) + num(cr_aug[\"RCON1403\"])\n",
    "cr_aug[\"nonperforming_loans\"] = npl_fd.combine_first(npl_con)\n",
    "\n",
    "loan_fd  = num(cr_aug[\"RCFD5369\"]) + num(cr_aug[\"RCFDB528\"])\n",
    "loan_con = num(cr_aug[\"RCON5369\"]) + num(cr_aug[\"RCONB528\"])\n",
    "cr_aug[\"total_loans\"] = loan_fd.combine_first(loan_con)\n",
    "\n",
    "# HTM, AFS, assets, capital -------------------------------------------------\n",
    "cr_aug[\"htm\"]  = num(cr_aug[\"RCFDJJ34\"]).combine_first(num(cr_aug[\"RCONJJ34\"]))\n",
    "cr_aug[\"htm_book\"] = num(cr_aug[\"RCFD1754\"]).combine_first(num(cr_aug[\"RCON1754\"]))\n",
    "cr_aug[\"unrealized_htm_losses\"] = (\n",
    "    num(cr_aug[\"RCFD1754\"]).combine_first(num(cr_aug[\"RCON1754\"]))\n",
    "    - num(cr_aug[\"RCFD1771\"]).combine_first(num(cr_aug[\"RCON1771\"]))\n",
    ")\n",
    "\n",
    "cr_aug[\"assets\"] = num(cr_aug[\"RCFD2170\"]).combine_first(num(cr_aug[\"RCON2170\"]))\n",
    "cr_aug[\"tier1_capital\"] = num(cr_aug[\"RCFA8274\"]).combine_first(num(cr_aug[\"RCOA8274\"]))\n",
    "cr_aug[\"afs\"] = num(cr_aug[\"RCFD1773\"]).combine_first(num(cr_aug[\"RCON1773\"]))\n",
    "\n",
    "# keep only columns R code keeps -------------------------------------------\n",
    "cr_keep = cr_aug[\n",
    "    [\n",
    "        \"tier1_capital\", \"cash_securities\", \"unrealized_htm_losses\",\n",
    "        \"cash\", \"assets\", \"afs\", \"htm\", \"htm_book\", \"securities\",\n",
    "        \"liabilties\", \"IDRSSD\", \"nonperforming_loans\", \"total_loans\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# 8-B.  Merge with the bank-sample and compute ratios ----------------------\n",
    "call_report_subsample_2022q4 = (\n",
    "    bank_sample\n",
    "      .query(\"dt_end >= 20210930\")                 # ≥ 2021-Q3\n",
    "      .merge(cr_keep, left_on=\"entity\", right_on=\"IDRSSD\", how=\"inner\")\n",
    "      .assign(\n",
    "          htm_ratio                     = lambda df: df.htm / df.assets,\n",
    "          htm_ratio2                    = lambda df: df.htm / df.afs,\n",
    "          npl_ratio                     = lambda df: df.nonperforming_loans / df.total_loans,\n",
    "          unrealized_htm_losses_rat_book  = lambda df: df.unrealized_htm_losses / df.htm_book,\n",
    "          unrealized_htm_losses_rat_tier1 = lambda df: df.unrealized_htm_losses / df.tier1_capital,\n",
    "          tier1capratio_cr              = lambda df: df.tier1_capital / df.assets,\n",
    "          liquid_asset_ratio            = lambda df: df.cash_securities / df.assets,\n",
    "          liquid_asset_ratio2           = lambda df: df.cash_securities / df.tier1_capital,\n",
    "          cash_asset_ratio              = lambda df: df.cash / df.assets,\n",
    "          securities_asset_ratio        = lambda df: df.securities / df.assets,\n",
    "          loans_asset_ratio             = lambda df: df.total_loans / df.assets,\n",
    "      )\n",
    "      .loc[lambda df: df.assets.notna()]           # drop rows missing total assets\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "#! This part can be done nicely with our dictionary! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. FDIC subsidiary-parent roll-up ------------------------------------\n",
    "# 9-A. Load the RSSD relationship file and keep only ‘live’ links ----------\n",
    "rel_all = (\n",
    "    pd.read_csv(\n",
    "        BASE_DIR / \"data\" / \"input\" / \"CSV_RELATIONSHIPS.csv\",\n",
    "        encoding=\"latin1\",\n",
    "        dtype=str          # keep everything as text first\n",
    "    )\n",
    ")\n",
    "\n",
    "# Turn the yyyymmdd string into a Timestamp column\n",
    "rel_all[\"DT_END\"] = pd.to_datetime(rel_all[\"DT_END\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "# Keep relationships that were still valid after 1 Jan 2023\n",
    "rel_all = rel_all[rel_all[\"DT_END\"] > \"2023-01-01\"]\n",
    "\n",
    "# remove the '#' from the column names:\n",
    "rel_all.columns = rel_all.columns.str.replace('#', '', regex=False)\n",
    "\n",
    "rel_all['ID_RSSD_PARENT'] = rel_all['ID_RSSD_PARENT'].astype(int)\n",
    "rel_all['ID_RSSD_OFFSPRING'] = rel_all['ID_RSSD_OFFSPRING'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-B. Keep only the parents that are in our bank sample -------------------\n",
    "relationship_data = (\n",
    "    bank_sample[[\"entity\"]]                       # parent RSSDIDs we care about\n",
    "      .merge(rel_all,\n",
    "             left_on=\"entity\",                   # parent in bank_sample\n",
    "             right_on=\"ID_RSSD_PARENT\",          # parent in relationship file\n",
    "             how=\"inner\")\n",
    "       .drop(columns='ID_RSSD_PARENT')  # drop the duplicate parent column\n",
    "       .rename(columns={\"entity\": \"ID_RSSD_PARENT\"})   # match R’s `select(ID_RSSD_PARENT = entity, …)`\n",
    "       .loc[:, [\"ID_RSSD_PARENT\", \"ID_RSSD_OFFSPRING\"]]\n",
    ")\n",
    "\n",
    "# 9-C. Add the convenience column `top_holder` -----------------------------\n",
    "relationship_data[\"top_holder\"] = relationship_data[\"ID_RSSD_PARENT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9-D.  Add the subsidiaries-of-subsidiaries ---------------------------\n",
    "\n",
    "# 1. Take every OFFSPRING we already have and treat it as a new “parent”\n",
    "child_relationship_data = (\n",
    "    relationship_data\n",
    "        .loc[:, [\"ID_RSSD_OFFSPRING\", \"top_holder\"]]        # keep the pair\n",
    "        .rename(columns={\"ID_RSSD_OFFSPRING\": \"entity\"})    # tmp name to match merge key\n",
    "        .merge(                                            # find its own children\n",
    "            rel_all,                                       # the full live-relationship table\n",
    "            left_on=\"entity\", right_on=\"ID_RSSD_PARENT\",\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        .loc[:, [\"ID_RSSD_OFFSPRING\", \"top_holder\"]]       # final shape\n",
    ")\n",
    "\n",
    "# 2. Append the new rows to the one-level relationship table\n",
    "relationship_data = pd.concat(\n",
    "    [\n",
    "        relationship_data.loc[:, [\"top_holder\", \"ID_RSSD_OFFSPRING\"]],\n",
    "        child_relationship_data\n",
    "    ],\n",
    "    ignore_index=True\n",
    ").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Build a complete charter-to-top_holder map by walking the hierarchy\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# relationship_data    <- contains the first two levels we already built\n",
    "# rel_all              <- full, *live* parent-child table (DT_END > 2023-01-01)\n",
    "\n",
    "working = relationship_data.copy()                           # start queue\n",
    "\n",
    "for _ in range(25):                                           # depth limit\n",
    "    # 1. use current offspring as new \"parents\"\n",
    "    new_kids = (\n",
    "        working[[\"ID_RSSD_OFFSPRING\", \"top_holder\"]]\n",
    "          .rename(columns={\"ID_RSSD_OFFSPRING\": \"entity\"})\n",
    "          .merge(\n",
    "              rel_all[[\"ID_RSSD_PARENT\", \"ID_RSSD_OFFSPRING\"]],\n",
    "              left_on=\"entity\",\n",
    "              right_on=\"ID_RSSD_PARENT\",\n",
    "              how=\"inner\"\n",
    "          )\n",
    "          .loc[:, [\"ID_RSSD_OFFSPRING\", \"top_holder\"]]\n",
    "    )\n",
    "\n",
    "    # 2. append and de-duplicate\n",
    "    before = len(relationship_data)\n",
    "    relationship_data = (\n",
    "        pd.concat(\n",
    "            [relationship_data[[\"top_holder\", \"ID_RSSD_OFFSPRING\"]], new_kids],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    # 3. stop early if we didn't find any new rows\n",
    "    if len(relationship_data) == before:\n",
    "        break\n",
    "\n",
    "    # 4. next iteration searches children of the *new* kids\n",
    "    working = new_kids\n",
    "\n",
    "# After the loop -----------------------------------------------------------\n",
    "# relationship_data now holds *every* charter-level RSSDID that rolls up\n",
    "# to one of the top-holders in your study, regardless of ownership depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume `relationship_data` already has the child links\n",
    "# and `bank_sample[\"entity\"]` holds the list of top-holder RSSDIDs.\n",
    "\n",
    "self_loops = (\n",
    "    bank_sample[[\"entity\"]]\n",
    "      .rename(columns={\"entity\": \"top_holder\"})\n",
    "      .assign(ID_RSSD_OFFSPRING=lambda df: df[\"top_holder\"])\n",
    ")\n",
    "\n",
    "relationship_data = (\n",
    "    pd.concat([relationship_data, self_loops], ignore_index=True)\n",
    "      .drop_duplicates()          # avoid doubles if already present\n",
    ")\n",
    "# This guarantees that every top-holder's balances will also enter the group by sum alongside its subsidiaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
